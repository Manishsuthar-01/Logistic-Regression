{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNWRA7rjdaJkhPwLU/vKbXn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Manishsuthar-01/Logistic-Regression/blob/main/Logistic_Regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. What is Logistic Regression, and how does it differ from Linear\n",
        "Regression?**\n",
        ">**Introduction**\n",
        "\n",
        ">Logistic Regression and Linear Regression are both statistical methods used in supervised machine learning. However, they serve different purposes and are used for different types of problems.\n",
        "\n",
        ">**Definition**\n",
        "\n",
        ">>**Linear Regression:**\n",
        "\n",
        ">>Linear Regression is used for predicting a continuous output.\n",
        "\n",
        ">>It establishes a linear relationship between the input variables (independent variables) and the output (dependent variable).\n",
        "\n",
        ">>The model predicts values using the equation:\n",
        "\n",
        ">>>$y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\cdots + \\beta_n x_n + \\varepsilon$\n",
        "\n",
        "\n",
        "\n",
        ">>where y is the predicted value, xi are the input features, βi are the coefficients, and ε is the error term.\n",
        "\n",
        ">**Logistic Regression:**\n",
        "\n",
        ">>Logistic Regression is used for classification problems, especially binary classification (e.g., spam vs. not spam).\n",
        "\n",
        ">>It predicts the probability that a given input belongs to a certain class (usually 0 or 1).\n",
        "\n",
        ">>The output is transformed using the logistic (sigmoid) function:\n",
        "\n",
        "$$\n",
        "P(y = 1 \\mid x) = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 x_1 + \\cdots + \\beta_n x_n)}}\n",
        "$$\n",
        "\n",
        "\n",
        ">>This maps the output to a range between 0 and 1.\n",
        "\n",
        ">**Example**\n",
        "\n",
        ">>**Linear Regression Example:**\n",
        "\n",
        ">>Predicting house prices based on size, location, and number of rooms.\n",
        "\n",
        ">>**Logistic Regression Example:**\n",
        "\n",
        ">>Predicting whether an email is spam (1) or not spam (0) based on its content.\n",
        "\n",
        ">**Assumptions**\n",
        "\n",
        ">>**Linear Regression assumes** linearity, independence, homoscedasticity, and normality of errors.\n",
        "\n",
        ">>**Logistic Regression assumes:**\n",
        "\n",
        ">>>The log-odds of the dependent variable is a linear combination of the independent variables.\n",
        "\n",
        ">>>No multicollinearity.\n",
        "\n",
        ">>>Large sample size for stable estimates.\n",
        "\n",
        ">**Applications**\n",
        "\n",
        ">>**Linear Regression:** Forecasting sales, temperature prediction, stock prices.\n",
        "\n",
        ">>**Logistic Regression:** Medical diagnosis, credit scoring, email spam detection, fraud detection."
      ],
      "metadata": {
        "id": "M_Zhnd-6EYOv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Explain the role of the Sigmoid function in Logistic Regression.**\n",
        ">In Logistic Regression, the Sigmoid function plays a central role in converting the output of a linear equation into a probability value between 0 and 1. This is essential for binary classification, where the goal is to determine whether an instance belongs to class 0 or class 1.\n",
        "\n",
        ">**What is the Sigmoid Function?**\n",
        "\n",
        ">The Sigmoid (or logistic) function is a mathematical function defined as:\n",
        "\n",
        "$$\n",
        "\\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
        "$$\n",
        "\n",
        "\n",
        ">Where:\n",
        "\n",
        ">$$\n",
        "z = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\cdots + \\beta_n x_n\n",
        "$$\n",
        "\n",
        "\n",
        ">>e is the base of the natural logarithm.\n",
        "\n",
        ">This function maps any real-valued number to a value in the range (0, 1).\n",
        "\n",
        ">**Why is it Needed in Logistic Regression?**\n",
        "\n",
        ">>Logistic Regression starts with a linear combination of input features (like in Linear Regression).\n",
        "\n",
        ">>However, unlike Linear Regression which outputs any real number, Logistic Regression needs to output probabilities.\n",
        "\n",
        ">>The Sigmoid function transforms the output of the linear model into a probability, making it suitable for classification tasks.\n",
        "\n",
        ">**How It Works**\n",
        "\n",
        ">>When z (the linear output) is a large positive number, σ(z) approaches 1.\n",
        "\n",
        ">>When z is a large negative number, σ(z) approaches 0.\n",
        "\n",
        ">>When z=0, σ(z)=0.5, representing the decision boundary.\n",
        "\n",
        ">**Decision Making**\n",
        "\n",
        ">>In binary classification:\n",
        "\n",
        ">>>If σ(z)≥0.5, predict class 1.\n",
        "\n",
        ">>>If σ(z)<0.5, predict class 0.\n",
        "\n",
        ">>This threshold (0.5 by default) can be adjusted based on the problem or evaluation metrics.\n",
        "\n",
        ">**Visualization Insight**\n",
        "\n",
        ">>The Sigmoid function has an S-shaped curve, which makes it ideal for modeling probabilistic transitions between classes, especially when the relationship is non-linear."
      ],
      "metadata": {
        "id": "Udx-pJ7JGPpe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. What is Regularization in Logistic Regression and why is it needed?**\n",
        ">**Introduction**\n",
        "\n",
        ">Regularization is a technique used in Logistic Regression (and other machine learning models) to prevent overfitting by penalizing large coefficients in the model. It helps the model to generalize better on unseen data.\n",
        "\n",
        ">**The Need for Regularization**\n",
        "\n",
        ">In Logistic Regression, the model learns coefficients 𝛽0,𝛽1,…,𝛽𝑛 to fit the training data. However:\n",
        "\n",
        ">>If the model is too complex or if there are too many features, it may learn patterns that are specific to the training data (i.e., noise).\n",
        "\n",
        ">>This results in overfitting, where the model performs well on training data but poorly on test data.\n",
        "\n",
        ">>Regularization addresses this by constraining the magnitude of the coefficients, encouraging simpler models.\n",
        "\n",
        ">**How Regularization Works**\n",
        "\n",
        ">>Regularization adds a penalty term to the loss function (cost function) that Logistic Regression tries to minimize.\n",
        "\n",
        ">>**Original Logistic Loss (Binary Cross-Entropy):**\n",
        "\n",
        "$$\n",
        "J(\\theta) = -\\frac{1}{m} \\sum_{i=1}^{m} \\left[ y^{(i)} \\log(h_{\\theta}(x^{(i)})) + (1 - y^{(i)}) \\log(1 - h_{\\theta}(x^{(i)})) \\right]\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        ">>**With Regularization:**\n",
        "\n",
        ">>>**L2 Regularization (Ridge)**– adds the squared magnitude of coefficients:\n",
        "\n",
        "$$\n",
        "J(\\theta) = \\text{Loss} + \\frac{\\lambda}{2m} \\sum_{j=1}^{n} \\theta_j^2\n",
        "$$\n",
        "\n",
        "\n",
        ">>>**L1 Regularization (Lasso)** – adds the absolute value of coefficients:\n",
        "\n",
        "$$\n",
        "J(\\theta) = \\text{Loss} + \\frac{\\lambda}{m} \\sum_{j=1}^{n} \\left| \\theta_j \\right|\n",
        "$$\n",
        "\n",
        "\n",
        ">>Where:\n",
        "\n",
        ">>λ is the regularization parameter that controls the strength of the penalty.\n",
        "\n",
        ">>θj ​are the model coefficients (excluding the bias term).\n",
        "\n",
        ">**Why Regularization is Important**\n",
        "\n",
        ">>Controls model complexity\n",
        "\n",
        ">>Reduces overfitting\n",
        "\n",
        ">>Improves generalization\n",
        "\n",
        ">>Helps with high-dimensional data (many features)\n",
        "\n",
        ">>Encourages simpler models with smaller weights"
      ],
      "metadata": {
        "id": "ACHh8N_xH4ux"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4. What are some common evaluation metrics for classification models, and\n",
        "why are they important?**\n",
        ">**Accuracy**\n",
        "\n",
        ">>**Definition:** The ratio of correctly predicted observations to the total observations.\n",
        "\n",
        ">>**Formula:**\n",
        "\n",
        "$$\n",
        "\\text{Accuracy} = \\frac{TP + TN}{TP + TN + FP + FN}\n",
        "$$\n",
        "\n",
        ">>**Importance:**\n",
        "\n",
        ">>>Simple and intuitive metric to measure overall correctness.\n",
        "\n",
        ">>>Useful when classes are balanced.\n",
        "\n",
        ">>**Limitation:**\n",
        "\n",
        ">>>Misleading if classes are imbalanced.\n",
        "\n",
        ">**Precision**\n",
        "\n",
        ">>**Definition:** The ratio of correctly predicted positive observations to the total predicted positives.\n",
        "\n",
        ">>**Formula:**\n",
        "$$\n",
        "\\text{Precision} = \\frac{TP}{TP + FP}\n",
        "$$\n",
        "\n",
        "\n",
        ">>**Importance:**\n",
        "\n",
        ">>>Important when the cost of false positives is high (e.g., spam detection, fraud detection).\n",
        "\n",
        ">>**Focus:**\n",
        "\n",
        ">>>How many selected items are relevant.\n",
        "\n",
        ">**Recall (Sensitivity or True Positive Rate)**\n",
        "\n",
        ">>**Definition:** The ratio of correctly predicted positive observations to all actual positives.\n",
        "\n",
        ">>**Formula:**\n",
        "$$\n",
        "\\text{Recall} = \\frac{TP}{TP + FN}\n",
        "$$\n",
        "\n",
        ">>**Importance:**\n",
        "\n",
        ">>>Crucial when the cost of false negatives is high (e.g., disease diagnosis).\n",
        "\n",
        ">>**Focus:**\n",
        "\n",
        ">>>How many relevant items are selected.\n",
        "\n",
        ">**F1 Score**\n",
        "\n",
        ">>**Definition:** The harmonic mean of Precision and Recall.\n",
        "\n",
        ">>**Formula:**\n",
        "\n",
        "$$\n",
        "F1 = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n",
        "$$\n",
        "\n",
        "\n",
        ">>**Importance:**\n",
        "\n",
        ">>>Balances Precision and Recall.\n",
        "\n",
        ">>>Useful when classes are imbalanced and you need a single metric.\n",
        "\n",
        ">**Confusion Matrix**\n",
        "\n",
        ">>**Definition:** A table showing the counts of True Positives (TP), True Negatives (TN), False Positives (FP), and False Negatives (FN).\n",
        "\n",
        ">>**Importance:**\n",
        "\n",
        ">>>Provides detailed insight into the types of errors.\n",
        "\n",
        ">>>Foundation for calculating other metrics.\n",
        "\n",
        ">**ROC Curve and AUC (Area Under the Curve)**\n",
        "\n",
        ">>**ROC Curve:** Plots True Positive Rate (Recall) against False Positive Rate at various threshold settings.\n",
        "\n",
        ">>**AUC:** Measures the overall ability of the model to discriminate between classes.\n",
        "\n",
        ">>**Importance:**\n",
        "\n",
        ">>>Useful for comparing models.\n",
        "\n",
        ">>>Works well with imbalanced datasets.\n"
      ],
      "metadata": {
        "id": "yJFjer9WJgv1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5. : Write a Python program that loads a CSV file into a Pandas DataFrame,\n",
        "splits into train/test sets, trains a Logistic Regression model, and prints its accuracy.**\n",
        "\n",
        "**(Use Dataset from sklearn package)**"
      ],
      "metadata": {
        "id": "h7MT-Xd9NcZD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 1: Load dataset from sklearn\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Step 2: Convert to Pandas DataFrame (optional, for visualization)\n",
        "df = pd.DataFrame(X, columns=iris.feature_names)\n",
        "df['target'] = y\n",
        "\n",
        "# Step 3: Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 4: Train a Logistic Regression model\n",
        "model = LogisticRegression(max_iter=200)  # Increase max_iter if needed\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 5: Make predictions and print accuracy\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Accuracy of the Logistic Regression model: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iQw5KtozNuQC",
        "outputId": "5a375d12-1d56-4621-9c8d-28a919742fe1"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the Logistic Regression model: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**6. Write a Python program to train a Logistic Regression model using L2\n",
        "regularization (Ridge) and print the model coefficients and accuracy.**\n",
        "\n",
        "**(Use Dataset from sklearn package)**"
      ],
      "metadata": {
        "id": "_Dc1vfu-N79l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 1: Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Step 2: Convert to DataFrame (optional)\n",
        "df = pd.DataFrame(X, columns=data.feature_names)\n",
        "df['target'] = y\n",
        "\n",
        "# Step 3: Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 4: Train Logistic Regression with L2 regularization (default)\n",
        "model = LogisticRegression(penalty='l2', solver='liblinear', C=1.0)  # C is the inverse of regularization strength\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 5: Print model coefficients\n",
        "print(\"Model Coefficients:\")\n",
        "for feature, coef in zip(data.feature_names, model.coef_[0]):\n",
        "    print(f\"{feature}: {coef:.4f}\")\n",
        "\n",
        "# Step 6: Evaluate accuracy\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"\\nAccuracy on test set: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GnGeHRczOIOy",
        "outputId": "49e78b0a-f2a8-4f71-e482-afc8ac6ec073"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Coefficients:\n",
            "mean radius: 2.1325\n",
            "mean texture: 0.1528\n",
            "mean perimeter: -0.1451\n",
            "mean area: -0.0008\n",
            "mean smoothness: -0.1426\n",
            "mean compactness: -0.4156\n",
            "mean concavity: -0.6519\n",
            "mean concave points: -0.3445\n",
            "mean symmetry: -0.2076\n",
            "mean fractal dimension: -0.0298\n",
            "radius error: -0.0500\n",
            "texture error: 1.4430\n",
            "perimeter error: -0.3039\n",
            "area error: -0.0726\n",
            "smoothness error: -0.0162\n",
            "compactness error: -0.0019\n",
            "concavity error: -0.0449\n",
            "concave points error: -0.0377\n",
            "symmetry error: -0.0418\n",
            "fractal dimension error: 0.0056\n",
            "worst radius: 1.2321\n",
            "worst texture: -0.4046\n",
            "worst perimeter: -0.0362\n",
            "worst area: -0.0271\n",
            "worst smoothness: -0.2626\n",
            "worst compactness: -1.2090\n",
            "worst concavity: -1.6180\n",
            "worst concave points: -0.6153\n",
            "worst symmetry: -0.7428\n",
            "worst fractal dimension: -0.1170\n",
            "\n",
            "Accuracy on test set: 0.9561\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**7. Write a Python program to train a Logistic Regression model for multiclass\n",
        "classification using multi_class='ovr' and print the classification report.**\n",
        "\n",
        "**(Use Dataset from sklearn package)**"
      ],
      "metadata": {
        "id": "QGoXc7a5OcEB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Step 1: Load dataset\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Step 2: Split into train/test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 3: Train Logistic Regression with OvR strategy\n",
        "model = LogisticRegression(multi_class='ovr', solver='liblinear', max_iter=200)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 4: Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Step 5: Print classification report\n",
        "print(\"Classification Report (OvR):\")\n",
        "print(classification_report(y_test, y_pred, target_names=data.target_names))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5j_K4iDJOmt5",
        "outputId": "e620323e-1be9-4fdb-c54e-a4df93583939"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report (OvR):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      setosa       1.00      1.00      1.00        10\n",
            "  versicolor       1.00      1.00      1.00         9\n",
            "   virginica       1.00      1.00      1.00        11\n",
            "\n",
            "    accuracy                           1.00        30\n",
            "   macro avg       1.00      1.00      1.00        30\n",
            "weighted avg       1.00      1.00      1.00        30\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**8. Write a Python program to apply GridSearchCV to tune C and penalty\n",
        "hyperparameters for Logistic Regression and print the best parameters and validation accuracy.**\n",
        "\n",
        "**(Use Dataset from sklearn package)**"
      ],
      "metadata": {
        "id": "rfgfL7w6Orgh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 1: Load the dataset\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Step 2: Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 3: Define the model\n",
        "model = LogisticRegression(multi_class='ovr', solver='liblinear')\n",
        "\n",
        "# Step 4: Define hyperparameter grid\n",
        "param_grid = {\n",
        "    'C': [0.01, 0.1, 1, 10, 100],          # Inverse of regularization strength\n",
        "    'penalty': ['l1', 'l2']                # L1 = Lasso, L2 = Ridge\n",
        "}\n",
        "\n",
        "# Step 5: Apply GridSearchCV\n",
        "grid_search = GridSearchCV(model, param_grid, cv=5, scoring='accuracy')\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Step 6: Print best parameters and validation score\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "print(\"Best Cross-Validation Accuracy: {:.4f}\".format(grid_search.best_score_))\n",
        "\n",
        "# Step 7: Evaluate on test data\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "test_accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Test Accuracy: {:.4f}\".format(test_accuracy))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wiOcKFf6O04x",
        "outputId": "541a56fc-7b37-476d-cc38-b736cb6d3cee"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/svm/_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'C': 10, 'penalty': 'l1'}\n",
            "Best Cross-Validation Accuracy: 0.9583\n",
            "Test Accuracy: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**9. Write a Python program to standardize the features before training Logistic\n",
        "Regression and compare the model's accuracy with and without scaling.**\n",
        "\n",
        "**(Use Dataset from sklearn package)**\n"
      ],
      "metadata": {
        "id": "xuHPuC3YO_HY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 1: Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Step 2: Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "### --- Model WITHOUT Scaling ---\n",
        "model_no_scaling = LogisticRegression(max_iter=1000)\n",
        "model_no_scaling.fit(X_train, y_train)\n",
        "y_pred_no_scaling = model_no_scaling.predict(X_test)\n",
        "acc_no_scaling = accuracy_score(y_test, y_pred_no_scaling)\n",
        "\n",
        "### --- Model WITH Scaling ---\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "model_scaled = LogisticRegression(max_iter=1000)\n",
        "model_scaled.fit(X_train_scaled, y_train)\n",
        "y_pred_scaled = model_scaled.predict(X_test_scaled)\n",
        "acc_scaled = accuracy_score(y_test, y_pred_scaled)\n",
        "\n",
        "# Step 3: Compare and Print Results\n",
        "print(f\"Accuracy WITHOUT scaling: {acc_no_scaling:.4f}\")\n",
        "print(f\"Accuracy WITH scaling   : {acc_scaled:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MYxVTgyjPLTo",
        "outputId": "ec9c5ef0-b1b5-4b36-cd66-d6898479c285"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy WITHOUT scaling: 0.9561\n",
            "Accuracy WITH scaling   : 0.9737\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**10. Imagine you are working at an e-commerce company that wants to\n",
        "predict which customers will respond to a marketing campaign. Given an imbalanced dataset (only 5% of customers respond), describe the approach you’d take to build a Logistic Regression model — including data handling, feature scaling, balancing classes, hyperparameter tuning, and evaluating the model for this real-world business use case.**\n",
        ">**1. Understand the Business Problem**\n",
        "\n",
        ">>**Goal:** Predict which customers are likely to respond to a marketing campaign.\n",
        "\n",
        ">>**Challenge:** Only 5% positive class, i.e., very imbalanced.\n",
        "\n",
        ">>**Cost-sensitive:** False negatives may lead to lost revenue; false positives may waste marketing spend.\n",
        "\n",
        ">**2. Data Preparation**\n",
        "\n",
        ">>Handle missing values (e.g., fill, drop, or impute).\n",
        "\n",
        ">>Convert categorical features using OneHotEncoder or pd.get_dummies().\n",
        "\n",
        ">>Split the data into training and test sets (e.g., train_test_split with stratify=y to preserve class ratio).\n",
        "\n",
        ">**3. Feature Scaling**\n",
        "\n",
        ">>Use StandardScaler to normalize features, especially for models like Logistic Regression that rely on feature magnitudes."
      ],
      "metadata": {
        "id": "xDcgTBN7POnf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n"
      ],
      "metadata": {
        "id": "dWLVTYJGPurf"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ">**4. Handle Class Imbalance**\n",
        "\n",
        ">>**Option A:** Use Class Weights (Recommended for Logistic Regression)"
      ],
      "metadata": {
        "id": "MupDs-tKPwTw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = LogisticRegression(class_weight='balanced')\n"
      ],
      "metadata": {
        "id": "oiSfoYJeP1_W"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ">>**Option B:** Resampling\n",
        "\n",
        ">>>Oversampling minority class: e.g., SMOTE()\n",
        "\n",
        ">>>Undersampling majority class: e.g., RandomUnderSampler()\n",
        "\n",
        ">>>Can use imblearn.pipeline.Pipeline for combining scaling + resampling + model training."
      ],
      "metadata": {
        "id": "lb3PnON2P4e2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ">**5. Train the Model**\n",
        "\n",
        ">>Use Logistic Regression with:\n",
        "\n",
        ">>>penalty='l2' (ridge regularization),\n",
        "\n",
        ">>>class_weight='balanced',\n",
        "\n",
        ">>>solver='liblinear' (if using L1/L2 on small data),\n",
        "\n",
        ">>>max_iter=1000.\n",
        "\n",
        ">**6. Hyperparameter Tuning with GridSearchCV**\n",
        "\n",
        ">>Tune:\n",
        "\n",
        ">>>C: regularization strength,\n",
        "\n",
        ">>>penalty: 'l1', 'l2'.\n",
        "\n",
        ">>Use GridSearchCV with scoring='f1' or scoring='roc_auc' instead of accuracy.\n",
        "\n",
        ">**7. Model Evaluation**\n",
        "\n",
        ">>Since accuracy is misleading in imbalanced datasets, use:\n",
        "\n",
        ">>**Metric:Why?**\n",
        "\n",
        ">>**Precision:**\tMinimize marketing waste (false positives)\n",
        "\n",
        ">>**Recall:**\tCapture as many responders as possible\n",
        "\n",
        ">>**F1-score:**\tBalance of precision and recall\n",
        "\n",
        ">>**ROC-AUC**:\tGeneral discrimination ability\n",
        "\n",
        ">>**PR AUC:**\tBetter for highly imbalanced classes\n",
        "\n",
        ">**8. Threshold Tuning**\n",
        "\n",
        ">>By default, predictions are based on threshold 0.5, but for imbalanced data:\n",
        "\n",
        ">>>Adjust threshold to improve recall/precision tradeoff.\n",
        "\n",
        ">>>Use precision-recall curve to choose optimal threshold.\n",
        "\n",
        ">**9. Model Deployment & Monitoring**\n",
        "\n",
        ">>Deploy the model (e.g., via API or in marketing pipeline).\n",
        "\n",
        ">>Monitor:\n",
        "\n",
        ">>>Precision/recall drift over time,\n",
        "\n",
        ">>>False positives/negatives,\n",
        "\n",
        ">>>Real campaign response vs. predictions."
      ],
      "metadata": {
        "id": "K4MJkBUnP9wO"
      }
    }
  ]
}